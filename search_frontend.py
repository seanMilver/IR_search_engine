from flask import Flask, request, jsonify
import re, pickle, math, pandas as pd
from collections import Counter
from operator import itemgetter
from google.cloud import storage
import nltk
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
import threading
import json
from functools import lru_cache

# --- GCP CONFIGURATION ---
PROJECT_ID = 'storied-epigram-480214-e9'
BUCKET_NAME = 'bucket_323040972'

# --- BUCKET PATHS ---
BODY_INDEX_BLOB = 'postings_gcp/index.pkl'
BODY_POSTINGS_PREFIX = 'postings_gcp'
TITLE_SHARDS_PREFIX = 'titles_shards'
DOCLEN_SHARDS_PREFIX = 'doclen_shards'
ANCHOR_INDEX_BLOB = 'postings_anchor_gcp/anchor_index.pkl'
ANCHOR_POSTINGS_PREFIX = 'postings_anchor_gcp'
TITLE_POSTINGS_PREFIX = 'postings_title_gcp'
TITLE_INDEX_BLOB = 'postings_title_gcp/title_index.pkl' 

_ANCHOR_INDEX = None
_TITLE_INDEX = None

from inverted_index_gcp import InvertedIndex

app = Flask(__name__)

# --- INITIALIZE TOOLS ---
stemmer = PorterStemmer()
nltk.download('stopwords')
english_stopwords = frozenset(stopwords.words('english'))
RE_WORD = re.compile(r"[\#\@\w](['\-]?\w){2,24}", re.UNICODE)

# --- GLOBALS ---
_STORAGE_CLIENT = storage.Client(project=PROJECT_ID)
_BUCKET = _STORAGE_CLIENT.bucket(BUCKET_NAME)
DOCLEN_DICT = {}  # For high-speed memory lookups

# PageRank loader
def load_pr():
    """Loads the PageRank CSV generated by Spark (pr/*.csv.gz)."""
    try:
        blobs = _BUCKET.list_blobs(prefix='pr/')
        dfs = [pd.read_csv(b.open("rb"), compression='gzip', names=['id', 'pr'])
               for b in blobs if b.name.endswith('.csv.gz')]
        if not dfs: return {}
        full_df = pd.concat(dfs)
        return dict(zip(full_df['id'], full_df['pr']))
    except Exception:
        return {}

PR_DICT = load_pr()

LIST_IDS = set()

def load_list_ids():
    """Identify documents that are lists or timelines from your shards."""
    global LIST_IDS
    print("Pre-scanning for list-type documents...")
    try:
        # Scan your title shards (similar to how you load doc_lens)
        blobs = list(_BUCKET.list_blobs(prefix=f'{TITLE_SHARDS_PREFIX}/'))
        for blob in blobs:
            if blob.name.endswith('.pkl'):
                shard_data = pickle.loads(blob.download_as_bytes())
                for doc_id, title in shard_data.items():
                    lower_title = title.lower()
                    # Mark IDs that match your 'drop' criteria
                    if lower_title.startswith("list of") or lower_title.startswith("timeline of"):
                        LIST_IDS.add(int(doc_id))
        print(f"Successfully indexed {len(LIST_IDS)} list-type articles.")
    except Exception as e:
        print(f"Error loading list filters: {e}")
load_list_ids()

def get_title_index():
    """Lazy-load the title index built in Spark."""
    global _TITLE_INDEX
    if _TITLE_INDEX is None:
        with _INDEX_LOCK:
            if _TITLE_INDEX is None:
                #
                _TITLE_INDEX = pickle.loads(_BUCKET.blob(TITLE_INDEX_BLOB).download_as_bytes())
    return _TITLE_INDEX

def get_anchor_index():
    """Lazy-load the anchor index built in Spark."""
    global _ANCHOR_INDEX
    if _ANCHOR_INDEX is None:
        with _INDEX_LOCK:
            if _ANCHOR_INDEX is None:
                _ANCHOR_INDEX = pickle.loads(_BUCKET.blob(ANCHOR_INDEX_BLOB).download_as_bytes())
    return _ANCHOR_INDEX

def load_all_doc_lengths():
    """Diagnostic loader to ensure DOCLEN_DICT is populated."""
    global DOCLEN_DICT
    print(f"--- ATTEMPTING TO LOAD DOCLENS FROM: {DOCLEN_SHARDS_PREFIX} ---")
    try:
        # 1. List all blobs in the prefix
        blobs = list(_BUCKET.list_blobs(prefix=f'{DOCLEN_SHARDS_PREFIX}/'))
        if not blobs:
            print(f"ERROR: No blobs found in 'gs://{BUCKET_NAME}/{DOCLEN_SHARDS_PREFIX}/'")
            return

        for blob in blobs:
            if blob.name.endswith('.pkl'):
                # 2. Download and load the dictionary shard
                shard_data = pickle.loads(blob.download_as_bytes())
                # 3. Force keys and values to standard Python integers
                for doc_id, length in shard_data.items():
                    DOCLEN_DICT[int(doc_id)] = int(length)
        
        # 4. Final verification
        print(f"SUCCESS: Loaded {len(DOCLEN_DICT)} document lengths.")
        if len(DOCLEN_DICT) > 0:
            sample_key = list(DOCLEN_DICT.keys())[0]
            print(f"SAMPLE ENTRY: doc_id {sample_key} has length {DOCLEN_DICT[sample_key]}")
            
    except Exception as e:
        print(f"CRITICAL SYSTEM ERROR: {str(e)}")
        
load_all_doc_lengths()

# ---- INDEX & DATA LOADERS ----
_BODY_INDEX = None
_INDEX_LOCK = threading.Lock()

def get_body_index():
    global _BODY_INDEX
    if _BODY_INDEX is None:
        with _INDEX_LOCK:
            if _BODY_INDEX is None:
                _BODY_INDEX = pickle.loads(_BUCKET.blob(BODY_INDEX_BLOB).download_as_bytes())
    return _BODY_INDEX

@lru_cache(maxsize=256)
def _load_title_shard(sid: int) -> dict:
    blob_name = f"{TITLE_SHARDS_PREFIX}/{sid:06d}.pkl"
    return pickle.loads(_BUCKET.blob(blob_name).download_as_bytes())

def get_real_title(doc_id: int) -> str:
    """Fast title lookup from sharded dictionaries."""
    sid = int(doc_id) // 100000
    try:
        shard = _load_title_shard(sid)
        return shard.get(int(doc_id), f"ID {doc_id}")
    except:
        return f"ID {doc_id}"

# ---- CORE FUNCTIONS ----
def tokenize(text, stem=False):
    tokens = [t.group() for t in RE_WORD.finditer(text.lower()) if t.group() not in english_stopwords]
    return [stemmer.stem(t) for t in tokens] if stem else tokens

def get_posting_list(index, token, postings_prefix):
    """Correctly unpacks bits (doc_id << 16 | tf) as defined in your indexer."""
    if token not in index.posting_locs:
        return []

    postings = []
    TUPLE_SIZE = 6
    TF_MASK = 2**16 - 1

    for file_name, offset in index.posting_locs[token]:
        blob = _BUCKET.blob(f'{postings_prefix}/{file_name}')
        df = index.df.get(token, 0)
        b = blob.download_as_bytes(start=offset, end=offset + (df * TUPLE_SIZE) - 1)
        
        for i in range(0, len(b), TUPLE_SIZE):
            packed = int.from_bytes(b[i:i+TUPLE_SIZE], 'big')
            doc_id = packed >> 16
            tf = packed & TF_MASK
            postings.append((doc_id, tf))
    return postings

# ---- MAIN SEARCH ROUTE ----
@app.route("/search")
def search():
    query = request.args.get('query', '')
    if not query: return jsonify({"results": []})

    W_TITLE, W_BODY = 0.7, 0.3
    LIST_PENALTY = 0.2  # Aggressive penalty to clear the top 100

    try:
        body_idx = get_body_index()
        raw_tokens = tokenize(query, stem=False)
        stemmed_q = tokenize(query, stem=True)
        q_counts = Counter(raw_tokens)
        q_mag = math.sqrt(len(set(stemmed_q)))

        # 1. RETRIEVAL: Body scoring with Instant Memory Penalty
        body_scores = Counter()
        for token, q_tf in q_counts.items():
            if token in body_idx.posting_locs:
                idf = math.log10(6348910 / body_idx.df.get(token, 1))
                for doc_id, tf in get_posting_list(body_idx, token, BODY_POSTINGS_PREFIX):
                    did_int = int(doc_id)
                    length = DOCLEN_DICT.get(did_int, 300)
                    
                    # Log-Normalization
                    norm_tf = tf / math.log10(length + 1) if length > 0 else tf
                    score_increment = (norm_tf * idf * (q_tf * idf))
                    
                    # INSTANT LOOKUP: No GCS call required here
                    if did_int in LIST_IDS:
                        score_increment *= LIST_PENALTY
                    
                    body_scores[did_int] += score_increment

        if not body_scores: return jsonify({"results": []})

        # The Top 100 is now filtered at high speed
        top_candidates = body_scores.most_common(100)
        max_body = top_candidates[0][1]

        # 2. RE-RANKING: Standard Title Scoring
        final_list = []
        for doc_id, b_score in top_candidates:
            raw_title = get_real_title(doc_id)
            stem_title = tokenize(raw_title, stem=True)
            dot = len(set(stemmed_q).intersection(set(stem_title)))
            t_mag = math.sqrt(len(set(stem_title)))
            title_cos = dot / (q_mag * t_mag) if (q_mag * t_mag) > 0 else 0

            total_score = (W_TITLE * title_cos) + (W_BODY * (b_score / max_body))
            final_list.append((str(doc_id), raw_title, total_score))

        sorted_res = sorted(final_list, key=itemgetter(2), reverse=True)[:30]
        return jsonify({"results": [[d, t] for d, t, s in sorted_res]})

    except Exception as e:
        return jsonify({"error": str(e)})
    
@app.route("/search_body")
def search_body():
    query = request.args.get('query', '')
    if not query:
        return jsonify([])

    try:
        index = get_body_index()
        tokens = tokenize(query, stem=False)
        scores = Counter()

        for token in tokens:
            if token in index.posting_locs:
                idf = math.log10(6348910 / index.df.get(token, 1))
                for doc_id, tf in get_posting_list(index, token, BODY_POSTINGS_PREFIX):
                    scores[doc_id] += (tf * idf)

        top30 = scores.most_common(30)
        return jsonify([(str(doc_id), get_real_title(doc_id)) for doc_id, _ in top30])
    except Exception:
        return jsonify([])

@app.route("/search_title")
def search_title():
    """Mandatory: Count DISTINCT query words in TITLE, NO stemming."""
    query = request.args.get('query', '')
    if not query:
        return jsonify([])

    try:
        tindex = get_title_index()
        counts = Counter()
        # Use set() for distinct words and stem=False as required
        tokens = set(tokenize(query, stem=False))
        
        for token in tokens:
            # Use bit-unpacking get_posting_list
            for doc_id, _ in get_posting_list(tindex, token, TITLE_POSTINGS_PREFIX):
                counts[doc_id] += 1

        top30 = counts.most_common(30)
        return jsonify([(str(doc_id), get_real_title(doc_id)) for doc_id, _ in top30])
    except Exception as e:
        return jsonify({"error": str(e)})

@app.route("/search_anchor")
def search_anchor():
    """Mandatory: Count DISTINCT query words in anchor text, NO stemming."""
    query = request.args.get('query', '')
    if not query:
        return jsonify([])

    try:
        # Using the dedicated anchor index built in Spark
        aindex = get_anchor_index()
        counts = Counter()
        tokens = set(tokenize(query, stem=False))
        
        for token in tokens:
            # Reversing bit-packing: (doc_id << 16 | tf)
            for doc_id, _ in get_posting_list(aindex, token, ANCHOR_POSTINGS_PREFIX):
                counts[doc_id] += 1

        top30 = counts.most_common(30)
        return jsonify([(str(doc_id), get_real_title(doc_id)) for doc_id, _ in top30])
    except Exception as e:
        return jsonify({"error": str(e)})

@app.route("/get_pagerank", methods=['POST'])
def get_pagerank():
    """Returns PageRank values for a list of document IDs."""
    ids = request.get_json()
    if not ids:
        return jsonify([])
    
    # Uses the PR_DICT loaded from your Spark CSV output
    res = [float(PR_DICT.get(int(wid), 0)) for wid in ids]
    return jsonify(res)

@app.route("/get_pageview", methods=['POST'])
def get_pageview():
    return jsonify([0] * len(request.get_json()))  # Placeholder

def run(**kwargs):
    """Allows se.run() call from your launcher code."""
    app.run(**kwargs)

if __name__ == '__main__':
    #load_all_doc_lengths() # Pre-load to avoid network latency during search
    app.run(host='0.0.0.0', port=8080)