from flask import Flask, request, jsonify
import re, pickle, math, pandas as pd
from collections import Counter
from operator import itemgetter
from google.cloud import storage
import nltk
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
import time
import threading
import json
from functools import lru_cache

# --- GCP CONFIGURATION ---
PROJECT_ID = 'storied-epigram-480214-e9'
BUCKET_NAME = 'bucket_323040972'

# --- BUCKET PATHS (UPDATE THESE IF YOUR NAMES ARE DIFFERENT) ---
BODY_INDEX_BLOB = 'postings_gcp/index.pkl'
BODY_POSTINGS_PREFIX = 'postings_gcp'

TITLE_INDEX_BLOB = 'postings_title_gcp/title_index.pkl'   # <-- change to 'postings_title_gcp/index.pkl' if that's what you uploaded
TITLE_POSTINGS_PREFIX = 'postings_title_gcp'

TITLE_SHARDS_PREFIX = 'titles_shards'   # folder containing 000000.pkl, 000001.pkl, ...
TITLE_META_BLOB = f'{TITLE_SHARDS_PREFIX}/_meta.json'     # optional
TITLE_SHARD_SIZE_FALLBACK = 100_000                      # must match how you built the shards

from inverted_index_gcp import InvertedIndex  # needed for pickle load on some setups

app = Flask(__name__)
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False

# --- INITIALIZE TOOLS ---
stemmer = PorterStemmer()
nltk.download('stopwords')
english_stopwords = frozenset(stopwords.words('english'))
RE_WORD = re.compile(r"[\#\@\w](['\-]?\w){2,24}", re.UNICODE)

# --- GLOBALS (loaded once per process) ---
_STORAGE_CLIENT = storage.Client(project=PROJECT_ID)
_BUCKET = _STORAGE_CLIENT.bucket(BUCKET_NAME)

# PageRank dict is optional (you already load it); keep as-is but reuse global bucket
def load_pr():
    """Loads the PageRank CSV generated by your Spark notebook (pr/*.csv.gz)."""
    try:
        blobs = _BUCKET.list_blobs(prefix='pr/')
        dfs = [pd.read_csv(b.open("rb"), compression='gzip', names=['id', 'pr'])
               for b in blobs if b.name.endswith('.csv.gz')]
        if not dfs:
            return {}
        full_df = pd.concat(dfs)
        return dict(zip(full_df['id'], full_df['pr']))
    except Exception:
        return {}

PR_DICT = load_pr()

# ---- INDEX LAZY LOADERS ----
_BODY_INDEX = None
_TITLE_INDEX = None
_INDEX_LOCK = threading.Lock()

def _load_pickle_from_gcs(blob_name: str):
    return pickle.loads(_BUCKET.blob(blob_name).download_as_bytes())

def get_body_index():
    """Lazy-load body index once per process (thread-safe)."""
    global _BODY_INDEX
    if _BODY_INDEX is None:
        with _INDEX_LOCK:
            if _BODY_INDEX is None:
                _BODY_INDEX = _load_pickle_from_gcs(BODY_INDEX_BLOB)
    return _BODY_INDEX

def get_title_index():
    """Lazy-load title index once per process (thread-safe)."""
    global _TITLE_INDEX
    if _TITLE_INDEX is None:
        with _INDEX_LOCK:
            if _TITLE_INDEX is None:
                _TITLE_INDEX = _load_pickle_from_gcs(TITLE_INDEX_BLOB)
    return _TITLE_INDEX

# ---- TITLES SHARDS (doc_id -> title) ----
_TITLE_SHARD_SIZE = None
_TITLE_SHARD_SIZE_LOCK = threading.Lock()

def get_title_shard_size() -> int:
    """Read shard size from _meta.json if present; otherwise use fallback."""
    global _TITLE_SHARD_SIZE
    if _TITLE_SHARD_SIZE is None:
        with _TITLE_SHARD_SIZE_LOCK:
            if _TITLE_SHARD_SIZE is None:
                try:
                    meta = json.loads(_BUCKET.blob(TITLE_META_BLOB).download_as_text())
                    _TITLE_SHARD_SIZE = int(meta.get('shard_size', TITLE_SHARD_SIZE_FALLBACK))
                except Exception:
                    _TITLE_SHARD_SIZE = TITLE_SHARD_SIZE_FALLBACK
    return _TITLE_SHARD_SIZE

@lru_cache(maxsize=256)  # caches shards (DATA cache, not query-results cache)
def _load_title_shard(sid: int) -> dict:
    blob_name = f"{TITLE_SHARDS_PREFIX}/{sid:06d}.pkl"
    return pickle.loads(_BUCKET.blob(blob_name).download_as_bytes())

def get_title(doc_id: int) -> str:
    """Fast title lookup from sharded {doc_id:title} stored in GCS."""
    shard_size = get_title_shard_size()
    sid = doc_id // shard_size
    try:
        shard = _load_title_shard(sid)
        return shard.get(doc_id, f"ID {doc_id}")
    except Exception:
        return f"ID {doc_id}"

# Keep name for compatibility with your existing code
def get_real_title(doc_id: int) -> str:
    return get_title(int(doc_id))

# ---- TOKENIZE ----
def tokenize(text, stem=False):
    tokens = [t.group() for t in RE_WORD.finditer(text.lower()) if t.group() not in english_stopwords]
    return [stemmer.stem(t) for t in tokens] if stem else tokens

# ---- POSTINGS READER ----
def get_posting_list(index, token, postings_prefix: str):
    """
    Reads posting list for token from GCS.
    postings_prefix: 'postings_gcp' (body) or 'postings_title_gcp' (title)
    """
    if token not in index.posting_locs:
        return []

    postings = []
    TUPLE_SIZE = 6  # doc_id (4 bytes) + tf (2 bytes)

    for file_name, offset in index.posting_locs[token]:
        blob = _BUCKET.blob(f'{postings_prefix}/{file_name}')
        df = index.df.get(token, 0)
        if df <= 0:
            continue

        b = blob.download_as_bytes(
            start=offset,
            end=offset + (df * TUPLE_SIZE) - 1
        )
        for i in range(0, len(b), TUPLE_SIZE):
            postings.append(
                (int.from_bytes(b[i:i+4], 'big'),
                 int.from_bytes(b[i+4:i+6], 'big'))
            )
    return postings

# --- ROUTES ---

@app.route("/search")
def search():
    query = request.args.get('query', '')
    if not query:
        return jsonify([])

    W_TITLE, W_BODY, W_PR = 0.6, 0.2, 0.2

    try:
        index = get_body_index()

        raw_tokens = tokenize(query, stem=False)
        stemmed_q = tokenize(query, stem=True)
        q_counts = Counter(raw_tokens)
        q_mag = math.sqrt(len(set(stemmed_q)))

        # 1) Body cosine candidates
        body_scores = Counter()
        for token, q_tf in q_counts.items():
            if token in index.posting_locs:
                idf = math.log10(6348910 / index.df.get(token, 1))
                for doc_id, tf in get_posting_list(index, token, BODY_POSTINGS_PREFIX):
                    body_scores[doc_id] += (tf * idf * (q_tf * idf))

        if not body_scores:
            return jsonify([])

        top_candidates = body_scores.most_common(50)
        max_body = top_candidates[0][1] if top_candidates else 1
        max_pr_val = math.log10(max(PR_DICT.values()) + 1) if PR_DICT else 1

        # 2) Title cosine using bucket titles (NO Wikipedia scraping)
        start_title_time = time.time()

        final = []
        for doc_id, b_score in top_candidates:
            raw_title = get_real_title(doc_id)
            stem_title = tokenize(raw_title, stem=True)

            dot = len(set(stemmed_q).intersection(set(stem_title)))
            title_mag = math.sqrt(len(set(stem_title)))
            title_cos = dot / (q_mag * title_mag) if (q_mag * title_mag) > 0 else 0

            pr_val = PR_DICT.get(doc_id, 0)
            norm_pr = math.log10(pr_val + 1) / max_pr_val if max_pr_val > 0 else 0

            norm_body = b_score / max_body
            total = (W_TITLE * title_cos) + (W_BODY * norm_body) + (W_PR * norm_pr)

            final.append((doc_id, total, raw_title))

        title_duration = round(time.time() - start_title_time, 3)

        # return top 30
        sorted_res = sorted(final, key=itemgetter(1), reverse=True)[:30]
        response = {
            "results": [[str(doc_id), title] for doc_id, _, title in sorted_res],
            "debug_metrics": {
                "title_lookup_seconds": title_duration,
                "candidate_count": len(top_candidates)
            }
        }
        return jsonify(response)

    except Exception as e:
        return jsonify({"error": str(e)})

@app.route("/search_body")
def search_body():
    query = request.args.get('query', '')
    if not query:
        return jsonify([])

    try:
        index = get_body_index()
        tokens = tokenize(query, stem=False)
        scores = Counter()

        for token in tokens:
            if token in index.posting_locs:
                idf = math.log10(6348910 / index.df.get(token, 1))
                for doc_id, tf in get_posting_list(index, token, BODY_POSTINGS_PREFIX):
                    scores[doc_id] += (tf * idf)

        top30 = scores.most_common(30)
        return jsonify([(str(doc_id), get_real_title(doc_id)) for doc_id, _ in top30])
    except Exception:
        return jsonify([])

@app.route("/search_title")
def search_title():
    """Mandatory: Count DISTINCT query words in TITLE, NO stemming."""
    query = request.args.get('query', '')
    if not query:
        return jsonify([])

    try:
        tindex = get_title_index()
        counts = Counter()
        for token in set(tokenize(query, stem=False)):
            for doc_id, _ in get_posting_list(tindex, token, TITLE_POSTINGS_PREFIX):
                counts[doc_id] += 1

        top30 = counts.most_common(30)
        return jsonify([(str(doc_id), get_real_title(doc_id)) for doc_id, _ in top30])
    except Exception as e:
        return jsonify({"error": str(e)})

@app.route("/search_anchor")
def search_anchor():
    """
    Mandatory: Count DISTINCT query words in anchor text, NO stemming.
    (If you also built an anchor index, load it exactly like get_title_index()
     and use its postings prefix. For now this keeps your original logic.)
    """
    query = request.args.get('query', '')
    if not query:
        return jsonify([])

    try:
        # NOTE: change this to your anchor index if you have one
        index = get_body_index()
        counts = Counter()
        for token in set(tokenize(query, stem=False)):
            for doc_id, _ in get_posting_list(index, token, BODY_POSTINGS_PREFIX):
                counts[doc_id] += 1

        top30 = counts.most_common(30)
        return jsonify([(str(doc_id), get_real_title(doc_id)) for doc_id, _ in top30])
    except Exception:
        return jsonify([])

@app.route("/get_pagerank", methods=['POST'])
def get_pagerank():
    ids = request.get_json()
    return jsonify([float(PR_DICT.get(int(wid), 0)) for wid in ids])

@app.route("/get_pageview", methods=['POST'])
def get_pageview():
    return jsonify([0] * len(request.get_json()))  # Placeholder

def run(**kwargs):
    """Allows se.run() call from your launcher code."""
    app.run(**kwargs)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
